{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#All Libaries are imported already#\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "\n",
    "# Code generated for \"Bill of Lading\" dataset registered under \"Bill of Lading (PIERS)\" package\n",
    "\n",
    "# Documentation and Installations Instructions link: https://catalogue.datalake.ihsmarkit.com (please follow the links to \"Documentation\")\n",
    "\n",
    "# This code is compatible with latest version of the Data Lake command line interface hosted on pypi.org: https://pypi.org/project/dli/\n",
    "\n",
    "# To run with python interpreter (preferably using 3.x version)\n",
    "\n",
    "####\n",
    "\n",
    "\n",
    "\n",
    "# Import Libraries\n",
    "\n",
    "import dli\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import os, glob\n",
    "from os import listdir\n",
    "import gc\n",
    "\n",
    "# Logon and Authenticate to the Data Lake\n",
    "\n",
    "\n",
    "print('#All Libaries are imported already#')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read useful columns only to gather all necessary rows and columns with matched tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proccessing the #1 file...\n",
      "Proccessing the #2 file...\n",
      "Proccessing the #3 file...\n",
      "Proccessing the #4 file...\n",
      "Proccessing the #5 file...\n",
      "Proccessing the #6 file...\n",
      "Proccessing the #7 file...\n",
      "Proccessing the #8 file...\n",
      "Proccessing the #9 file...\n",
      "Proccessing the #10 file...\n",
      "Proccessing the #11 file...\n",
      "Proccessing the #12 file...\n",
      "Proccessing the #13 file...\n",
      "Proccessing the #14 file...\n",
      "Proccessing the #15 file...\n",
      "Proccessing the #16 file...\n",
      "Proccessing the #17 file...\n",
      "Proccessing the #18 file...\n",
      "Proccessing the #19 file...\n",
      "Proccessing the #20 file...\n",
      "Proccessing the #21 file...\n",
      "Proccessing the #22 file...\n",
      "Proccessing the #23 file...\n",
      "Proccessing the #24 file...\n",
      "Proccessing the #25 file...\n",
      "Proccessing the #26 file...\n",
      "Proccessing the #27 file...\n",
      "Proccessing the #28 file...\n",
      "Proccessing the #29 file...\n",
      "Proccessing the #30 file...\n",
      "Proccessing the #31 file...\n",
      "Proccessing the #32 file...\n",
      "Proccessing the #33 file...\n",
      "Proccessing the #34 file...\n",
      "Proccessing the #35 file...\n",
      "Proccessing the #36 file...\n",
      "Proccessing the #37 file...\n",
      "Proccessing the #38 file...\n",
      "Proccessing the #39 file...\n",
      "Proccessing the #40 file...\n",
      "Proccessing the #41 file...\n",
      "Proccessing the #42 file...\n",
      "Proccessing the #43 file...\n",
      "Proccessing the #44 file...\n",
      "Proccessing the #45 file...\n",
      "Proccessing the #46 file...\n",
      "Proccessing the #47 file...\n"
     ]
    }
   ],
   "source": [
    "# BoL import data\n",
    "# Splitting all datasets and filtering them with common tickers to get a full list of useful datas -- Time elapsed ~2h\n",
    "BoL_ticker = 'matched_ntfcompany_ihsm_ticker'\n",
    "column_list = [BoL_ticker, 'harm4', 'harmcode', 'vyear','vmonth','qty', 'kilos', 'metric_tons', 'estimated_dollarvalue','foreign_company_country','foreign_company_name']\n",
    "tickers = pd.read_excel(rf'C:\\Users\\Thomas TH Chow\\Desktop\\Datalake\\Tickers\\IHS {BoL_ticker} Common List.xlsx')['Common Ticker'].to_list()\n",
    "dirName = r'C:\\Users\\Thomas TH Chow\\Desktop\\Datalake\\Credit Rating Modeling\\BOLPTYCMDJoined\\as_of_date=2020-06-08\\type=full'\n",
    "Bill_Files = [file for file in listdir(dirName) if file.endswith('.snappy.parquet')]\n",
    "\n",
    "for i in range (1, len(Bill_Files)):\n",
    "    # Splitting all datasets and filtering them with common tickers to get a full list of useful datas\n",
    "    Bill_File = Bill_Files[i]\n",
    "    print(f\"Proccessing the #{i} file...\")\n",
    "    output_path = rf\"C:\\Users\\Thomas TH Chow\\Desktop\\Datalake\\Credit Rating Modeling\\Clean datasets\\Export Data\\Bill of Lading\\Part{i}\"\n",
    "    dataset_path = os.path.join(dirName, Bill_File)\n",
    "    Temp_df = pd.read_parquet(dataset_path, columns = column_list)\n",
    "    Temp_df = Temp_df[Temp_df[BoL_ticker].isin(tickers)]\n",
    "    final_output = r\"C:\\Users\\Thomas TH Chow\\Desktop\\Datalake\\Credit Rating Modeling\\Clean datasets\\Import Data\\Bill of Lading\\Bill_df Part{}.parquet.gzip\".format(i)\n",
    "    Temp_df.to_parquet(final_output, compression='gzip')\n",
    "    del Temp_df\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-12-24 16:31:11.995687]Proccessing the #1 file...\n",
      "[2020-12-24 16:32:50.957076]Proccessing the #2 file...\n",
      "[2020-12-24 16:34:31.209381]Proccessing the #3 file...\n",
      "[2020-12-24 16:36:03.247439]Proccessing the #4 file...\n",
      "[2020-12-24 16:38:11.413434]Proccessing the #5 file...\n",
      "[2020-12-24 16:40:34.413184]Proccessing the #6 file...\n",
      "[2020-12-24 16:42:22.609455]Proccessing the #7 file...\n",
      "[2020-12-24 16:43:57.872611]Proccessing the #8 file...\n",
      "[2020-12-24 16:45:37.010043]Proccessing the #9 file...\n",
      "[2020-12-24 16:47:35.838606]Proccessing the #10 file...\n",
      "[2020-12-24 16:49:15.951084]Proccessing the #11 file...\n",
      "[2020-12-24 16:51:35.166167]Proccessing the #12 file...\n",
      "[2020-12-24 16:54:35.455360]Proccessing the #13 file...\n",
      "[2020-12-24 16:57:43.378040]Proccessing the #14 file...\n",
      "[2020-12-24 17:00:52.402496]Proccessing the #15 file...\n",
      "[2020-12-24 17:02:50.392893]Proccessing the #16 file...\n",
      "[2020-12-24 17:04:42.846279]Proccessing the #17 file...\n",
      "[2020-12-24 17:07:01.045085]Proccessing the #18 file...\n",
      "[2020-12-24 17:08:47.383083]Proccessing the #19 file...\n",
      "[2020-12-24 17:10:41.340435]Proccessing the #20 file...\n",
      "[2020-12-24 17:12:23.720664]Proccessing the #21 file...\n",
      "[2020-12-24 17:14:03.485199]Proccessing the #22 file...\n",
      "[2020-12-24 17:15:48.508565]Proccessing the #23 file...\n",
      "[2020-12-24 17:19:12.966235]Proccessing the #24 file...\n",
      "[2020-12-24 17:21:29.560544]Proccessing the #25 file...\n",
      "[2020-12-24 17:23:11.062982]Proccessing the #26 file...\n",
      "[2020-12-24 17:25:01.838508]Proccessing the #27 file...\n",
      "[2020-12-24 17:27:08.566539]Proccessing the #28 file...\n",
      "[2020-12-24 17:29:19.286238]Proccessing the #29 file...\n",
      "[2020-12-24 17:32:14.862366]Proccessing the #30 file...\n",
      "[2020-12-24 17:34:00.188885]Proccessing the #31 file...\n",
      "[2020-12-24 17:35:44.330644]Proccessing the #32 file...\n",
      "[2020-12-24 17:37:52.381689]Proccessing the #33 file...\n",
      "[2020-12-24 17:39:33.945498]Proccessing the #34 file...\n",
      "[2020-12-24 17:41:24.564678]Proccessing the #35 file...\n",
      "[2020-12-24 17:43:47.250496]Proccessing the #36 file...\n",
      "[2020-12-24 17:45:48.085483]Proccessing the #37 file...\n",
      "[2020-12-24 17:47:42.209252]Proccessing the #38 file...\n",
      "[2020-12-24 17:49:14.969931]Proccessing the #39 file...\n",
      "[2020-12-24 17:51:04.101119]Proccessing the #40 file...\n",
      "[2020-12-24 17:52:56.513883]Proccessing the #41 file...\n",
      "[2020-12-24 17:55:19.177280]Proccessing the #42 file...\n",
      "[2020-12-24 17:57:28.895385]Proccessing the #43 file...\n",
      "[2020-12-24 18:00:01.928895]Proccessing the #44 file...\n",
      "[2020-12-24 18:03:07.107908]Proccessing the #45 file...\n",
      "[2020-12-24 18:06:10.132678]Proccessing the #46 file...\n",
      "[2020-12-24 18:09:07.370965]Proccessing the #47 file...\n"
     ]
    }
   ],
   "source": [
    "# BoL Export data\n",
    "# Splitting all datasets and filtering them with common tickers to get a full list of useful datas -- Time elapsed ~2h\n",
    "from datetime import datetime\n",
    "BoL_ticker = 'matched_fcompany_ihsm_ticker'\n",
    "column_list = [BoL_ticker, 'harm4', 'harmcode', 'vyear','vmonth','qty', 'kilos', 'metric_tons', 'estimated_dollarvalue',\n",
    "               'foreign_company_country','us_company_name', 'matched_fcompany_industry','notify_company_name',\n",
    "               'matched_fcompany_sector',\n",
    "              'matched_ntfcompany_sector', 'matched_ntfcompany_industry']\n",
    "tickers = pd.read_excel(rf'C:\\Users\\Thomas TH Chow\\Desktop\\Datalake\\Tickers\\IHS {BoL_ticker} Common List.xlsx')['Common Ticker'].to_list()\n",
    "dirName = r'C:\\Users\\Thomas TH Chow\\Desktop\\Datalake\\Credit Rating Modeling\\BOLPTYCMDJoined\\as_of_date=2020-06-08\\type=full'\n",
    "Bill_Files = [file for file in listdir(dirName) if file.endswith('.snappy.parquet')]\n",
    "\n",
    "for i in range (1, len(Bill_Files)):\n",
    "    # Splitting all datasets and filtering them with common tickers to get a full list of useful datas\n",
    "    Bill_File = Bill_Files[i]\n",
    "    print(rf\"[{datetime.now()}]Proccessing the #{i} file...\")\n",
    "    dataset_path = os.path.join(dirName, Bill_File)\n",
    "    Temp_df = pd.read_parquet(dataset_path, columns = column_list)\n",
    "    Temp_df = Temp_df[Temp_df[BoL_ticker].isin(tickers)]\n",
    "    final_output = r\"C:\\Users\\Thomas TH Chow\\Desktop\\Datalake\\Credit Rating Modeling\\Clean datasets\\Export Data\\Bill of Lading\\Bill_export Part{}.parquet.gzip\".format(i)\n",
    "    Temp_df.to_parquet(final_output, compression='gzip')\n",
    "    del Temp_df\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808\n"
     ]
    }
   ],
   "source": [
    "print(len(tickers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
