{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#All Libaries Imported#\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "\n",
    "# Code generated for \"Bill of Lading\" dataset registered under \"Bill of Lading (PIERS)\" package\n",
    "\n",
    "# Documentation and Installations Instructions link: https://catalogue.datalake.ihsmarkit.com (please follow the links to \"Documentation\")\n",
    "\n",
    "# This code is compatible with latest version of the Data Lake command line interface hosted on pypi.org: https://pypi.org/project/dli/\n",
    "\n",
    "# To run with python interpreter (preferably using 3.x version)\n",
    "\n",
    "####\n",
    "\n",
    "\n",
    "\n",
    "# Import Libraries\n",
    "\n",
    "import dli\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import os, glob\n",
    "from os import listdir\n",
    "import gc\n",
    "from datetime import datetime\n",
    "\n",
    "print('#All Libaries Imported#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64435, 19)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concat all trial output files to one dataframe\n",
    "input_path = r\"C:\\Users\\Thomas TH Chow\\Desktop\\Datalake\\Credit Rating Modeling\\Clean datasets\\Export Data\\Model Input Trial\"\n",
    "Trial_files = [file for file in listdir(input_path) if file.endswith('.xlsx')]\n",
    "Imput_list = ['harm4','foreign_company_country','Max_qty_us_company_name','Max_estimated_dollarvalue_us_company_name']\n",
    "for i in range (0,len(Trial_files)):\n",
    "    path = os.path.join(input_path, Trial_files[i])\n",
    "    Temp_df = pd.read_excel(path,index_col=[0,1])\n",
    "    # Impute the column with too many zeros with the company level mean of column\n",
    "    for column in Imput_list:\n",
    "        Temp_df[column].replace({0:np.nan},inplace=True)\n",
    "    Temp_df.fillna(Temp_df.mean(), inplace = True)\n",
    "    # Remove data after 2020 jun as the bill of lading data are not available after which\n",
    "    for j in range(7,12):\n",
    "        try:\n",
    "            Temp_df.drop(index = (2020,j), inplace = True)\n",
    "        except:\n",
    "            continue\n",
    "    Temp_df = Temp_df.reset_index(drop = True)\n",
    "    if i==0:\n",
    "        Trial_df = Temp_df\n",
    "    else:\n",
    "        Trial_df = Trial_df.append(Temp_df, sort = False)\n",
    "Trial_df = Trial_df.reset_index(drop = True)\n",
    "Trial_df.drop(columns = ['foreign_company_country'], inplace = True)\n",
    "# Impute the column with nan with 0\n",
    "Trial_df.fillna(0, inplace = True)\n",
    "Trial_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198, 19)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trial_df[Trial_df['Default']==True].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output correlation matrix of all columns with Default and PD to check linear correlation\n",
    "corr_matrix = Trial_df.corr()\n",
    "corr_matrix[[\"Default\",\"PD\"]].sort_values(by=['Default'], ascending = False).to_excel('Correlation Matrix_Default.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:51454 Test size:12981\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "Normal = ['days_since_last_qty',\n",
    "          'estimated_dollarvalue_12Mvol',\n",
    "          'estimated_dollarvalue_SMA12_log10', 'estimated_dollarvalue_SMA36_log10',\n",
    "          'Unit_price_qty_SMA12_log10','Unit_price_qty_SMA36_log10',\n",
    "          'harm4']\n",
    "         \n",
    "Std = ['estimated_dollarvalue_SMA36','estimated_dollarvalue_SMA12','Unit_price_qty_SMA36',\n",
    "      'Unit_price_qty_SMA12','Osci_estimated_dollarvalue_SMA12_estimated_dollarvalue_SMA36']\n",
    "\n",
    "Trial_Normal = Trial_df[Normal]\n",
    "Trial_Std = Trial_df[Std]\n",
    "Trial_other = Trial_df.drop(Normal+Std+[\"Default\",\"PD\"],axis = 1)\n",
    "Trial_labels = Trial_df[[\"Default\",\"PD\"]].copy()\n",
    "\n",
    "#Normalise selected columns above\n",
    "Normal_pipeline = Pipeline([\n",
    "('Normal_scaler', Normalizer()),\n",
    "])\n",
    "\n",
    "#Standardise selected columns above\n",
    "Std_pipeline = Pipeline([\n",
    "('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "# Implement normalisation and standardisation with pipeline\n",
    "normX = Normal_pipeline.fit_transform(Trial_Normal)\n",
    "normX_df = pd.DataFrame(normX, columns=Normal)\n",
    "stdX = Std_pipeline.fit_transform(Trial_Std)\n",
    "stdX_df = pd.DataFrame(stdX, columns=Std)\n",
    "Full_df = pd.concat([normX_df, stdX_df, Trial_other, Trial_labels], axis=1)\n",
    "\n",
    "# Split the training sets and testing sets with random function\n",
    "train_split = np.random.rand(len(Full_df)) < 0.8\n",
    "Trial_train = Full_df[train_split]\n",
    "Trial_test = Full_df[~train_split]\n",
    "print(f'Train size:{len(Trial_train)} Test size:{len(Trial_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156, 19)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export the training datasets parquet\n",
    "output_path = r\"C:\\Users\\Thomas TH Chow\\Desktop\\Datalake\\Credit Rating Modeling\\Clean datasets\\Export Data\\Trial training and testing datasets\"\n",
    "path = os.path.join(output_path, 'Trial Training Set.parquet.gzip')\n",
    "Trial_train.to_parquet(path,compression='gzip')\n",
    "Trial_train[Trial_train[\"Default\"]==True].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 19)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export the testing datasets parquet\n",
    "output_path = r\"C:\\Users\\Thomas TH Chow\\Desktop\\Datalake\\Credit Rating Modeling\\Clean datasets\\Export Data\\Trial training and testing datasets\"\n",
    "path = os.path.join(output_path, 'Trial Testing Set.parquet.gzip')\n",
    "Trial_test.to_parquet(path,compression='gzip')\n",
    "Trial_test[Trial_test[\"Default\"]==True].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(960, 19)\n",
      "(253, 19)\n"
     ]
    }
   ],
   "source": [
    "print(Trial_train[Trial_train[\"PD\"]>0.1].shape)\n",
    "print(Trial_test[Trial_test[\"PD\"]>0.1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finished the features engineering and Proceed to model fitting and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0.01294498 0.01371997 0.01335651 0.01330047 0.01381048]\n",
      "Labels: [0.003348830118225732, 0.003527181495977083, 0.003774126680097245, 0.0037194821923342, 0.003477256957170827]\n"
     ]
    }
   ],
   "source": [
    "# Linear regression model fitting\n",
    "Train = Trial_train.drop([\"Default\",\"PD\",\"Ticker\"],axis = 1).to_numpy()\n",
    "Train_labels = Trial_train[\"PD\"].copy().to_numpy()\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(Train, Train_labels)\n",
    "\n",
    "some_data = Trial_train.drop([\"Default\",\"PD\",\"Ticker\"],axis = 1).iloc[:5].to_numpy()\n",
    "some_labels = Trial_train[\"PD\"].iloc[:5]\n",
    "print(\"Predictions:\", lin_reg.predict(some_data))\n",
    "print(\"Labels:\", list(some_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Public\\Anaconda3\\lib\\site-packages\\xlsxwriter\\workbook.py\", line 320, in close\n",
      "    self._store_workbook()\n",
      "  File \"C:\\Users\\Public\\Anaconda3\\lib\\site-packages\\xlsxwriter\\workbook.py\", line 638, in _store_workbook\n",
      "    raise e\n",
      "  File \"C:\\Users\\Public\\Anaconda3\\lib\\site-packages\\xlsxwriter\\workbook.py\", line 635, in _store_workbook\n",
      "    xlsx_file = ZipFile(self.filename, \"w\", compression=ZIP_DEFLATED,\n",
      "  File \"C:\\Users\\Public\\Anaconda3\\lib\\zipfile.py\", line 1250, in __init__\n",
      "    self.fp = io.open(file, filemode)\n",
      "PermissionError: [Errno 13] Permission denied: 'Linregmodel Result.xlsx'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Public\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3418, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-10-62a6b68bf659>\", line 6, in <module>\n",
      "    Lin_result_df.to_excel('Linregmodel Result.xlsx')\n",
      "  File \"C:\\Users\\Public\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\", line 2026, in to_excel\n",
      "    formatter.write(\n",
      "  File \"C:\\Users\\Public\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\excel.py\", line 742, in write\n",
      "    writer.save()\n",
      "  File \"C:\\Users\\Public\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_xlsxwriter.py\", line 193, in save\n",
      "    return self.book.close()\n",
      "  File \"C:\\Users\\Public\\Anaconda3\\lib\\site-packages\\xlsxwriter\\workbook.py\", line 322, in close\n",
      "    raise FileCreateError(e)\n",
      "xlsxwriter.exceptions.FileCreateError: [Errno 13] Permission denied: 'Linregmodel Result.xlsx'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Public\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'FileCreateError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Public\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1170, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\Public\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Public\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\Public\\Anaconda3\\lib\\inspect.py\", line 1503, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\Public\\Anaconda3\\lib\\inspect.py\", line 1461, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\Public\\Anaconda3\\lib\\inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\Public\\Anaconda3\\lib\\inspect.py\", line 754, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"C:\\Users\\Public\\Anaconda3\\lib\\ntpath.py\", line 664, in realpath\n",
      "    if _getfinalpathname(spath) == path:\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\Public\\Anaconda3\\lib\\site-packages\\xlsxwriter\\workbook.py\u001b[0m in \u001b[0;36mclose\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    319\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_store_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\Anaconda3\\lib\\site-packages\\xlsxwriter\\workbook.py\u001b[0m in \u001b[0;36m_store_workbook\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    637\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 638\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    639\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\Anaconda3\\lib\\site-packages\\xlsxwriter\\workbook.py\u001b[0m in \u001b[0;36m_store_workbook\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    634\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m             xlsx_file = ZipFile(self.filename, \"w\", compression=ZIP_DEFLATED,\n\u001b[0m\u001b[0;32m    636\u001b[0m                                 allowZip64=self.allow_zip64)\n",
      "\u001b[1;32mC:\\Users\\Public\\Anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[0;32m   1249\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1250\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1251\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'Linregmodel Result.xlsx'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileCreateError\u001b[0m                           Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-62a6b68bf659>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mLin_result_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTrial_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Default\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPD_linprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"PD_linprediction\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mLin_result_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Linregmodel Result.xlsx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Public\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_excel\u001b[1;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, encoding, inf_rep, verbose, freeze_panes)\u001b[0m\n\u001b[0;32m   2025\u001b[0m         )\n\u001b[1;32m-> 2026\u001b[1;33m         formatter.write(\n\u001b[0m\u001b[0;32m   2027\u001b[0m             \u001b[0mexcel_writer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\excel.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine)\u001b[0m\n\u001b[0;32m    741\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mneed_save\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 742\u001b[1;33m             \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Public\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_xlsxwriter.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    192\u001b[0m         \"\"\"\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\Anaconda3\\lib\\site-packages\\xlsxwriter\\workbook.py\u001b[0m in \u001b[0;36mclose\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mFileCreateError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLargeZipFile\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileCreateError\u001b[0m: [Errno 13] Permission denied: 'Linregmodel Result.xlsx'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\Public\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2044\u001b[0m                         \u001b[1;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2045\u001b[1;33m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2046\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'FileCreateError' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2045\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2046\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2047\u001b[1;33m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[0;32m   2048\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[0;32m   2049\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1434\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1435\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1436\u001b[1;33m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1437\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0;32m   1438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1334\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1335\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1336\u001b[1;33m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1337\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1338\u001b[0m             )\n",
      "\u001b[1;32mC:\\Users\\Public\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1191\u001b[0m         \u001b[1;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1193\u001b[1;33m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[0;32m   1194\u001b[0m                                                                tb_offset)\n\u001b[0;32m   1195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1151\u001b[1;33m         \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[1;34m(etype, value, records)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[1;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 451\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# Linear regression model testing and export to excel for discussion\n",
    "Test = Trial_test.drop([\"Default\",\"PD\",\"Ticker\"],axis = 1).to_numpy()\n",
    "Test_labels = Trial_test[\"PD\"].copy().to_numpy()\n",
    "PD_linprediction = lin_reg.predict(Test)\n",
    "Lin_result_df = pd.concat([Trial_test.drop([\"Default\"],axis = 1).reset_index(drop=True), pd.DataFrame(PD_linprediction, columns=[\"PD_linprediction\"]).reset_index(drop=True)], axis = 1)\n",
    "Lin_result_df.to_excel('Linregmodel Result.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree regression fitting\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg = DecisionTreeRegressor(max_depth=10)\n",
    "tree_reg.fit(Train, Train_labels)\n",
    "Train_labels2 = Train_labels - tree_reg.predict(Train)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=10)\n",
    "tree_reg2.fit(Train, Train_labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Decision Tree regression model testing and export to excel for discussion\n",
    "Train = Trial_train.drop([\"Default\",\"PD\",\"Ticker\"],axis = 1).to_numpy()\n",
    "Train_labels = Trial_train[\"PD\"].copy().to_numpy()\n",
    "Train_labels.shape\n",
    "PD_treeprediction = sum(tree.predict(Train) for tree in (tree_reg, tree_reg2))\n",
    "\n",
    "Tree_result_df = pd.concat([Trial_train.drop([\"Default\"],axis = 1).reset_index(drop=True), pd.DataFrame(PD_treeprediction, columns=[\"PD_treeprediction\"]).reset_index(drop=True)], axis = 1)\n",
    "Tree_result_df.to_excel('Treeregmodel Result.xlsx')\n",
    "\n",
    "def Kendall_rank_Tree(Trainingdataset, Labels):\n",
    "    x1 = sum(tree.predict(Trainingdataset) for tree in (tree_reg, tree_reg2)).ravel()\n",
    "    x2 = Labels.ravel()\n",
    "    tau, p_value = stats.kendalltau(x1, x2)\n",
    "    return tau, p_value\n",
    "\n",
    "Kendall_tau, Kendall_p_value = Kendall_rank_Tree(Train, Train_labels)\n",
    "print(\"-----Training Sets------\")\n",
    "print(\"Kendall tau: \", Kendall_tau)\n",
    "print(\"Kendall p value: \", Kendall_p_value)\n",
    "Kendall_test_tau, Kendall_test_p_value = Kendall_rank_Tree(Test, Test_labels)\n",
    "print(\"-----Testing Sets------\")\n",
    "print(\"Kendall tau: \", Kendall_test_tau)\n",
    "print(\"Kendall p value: \", Kendall_test_p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, score in zip(Trial_train.drop([\"Default\",\"PD\",\"Ticker\"],axis = 1).columns, tree_reg.feature_importances_):\n",
    "    if score > 0.1:\n",
    "        print(name, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "print(tree_reg.feature_importances_)\n",
    "tree.plot_tree(tree_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Random Forest Forecast model fitting\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(Train, Train_labels)\n",
    "# print featrue importances\n",
    "for name, score in zip(Trial_train.drop([\"Default\",\"PD\",\"Ticker\"],axis = 1).columns, forest_reg.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Forecast model testing and export to excel for discussion\n",
    "Test = Trial_test.drop([\"Default\",\"PD\",\"Ticker\"],axis = 1).to_numpy()\n",
    "Test_labels = Trial_test[\"PD\"].copy().to_numpy()\n",
    "Test_labels.shape\n",
    "PD_forestprediction = forest_reg.predict(Test)\n",
    "\n",
    "Forest_result_df = pd.concat([Trial_test.drop([\"Default\"],axis = 1).reset_index(drop=True), pd.DataFrame(PD_forestprediction, columns=[\"PD_forestprediction\"]).reset_index(drop=True)], axis = 1)\n",
    "Forest_result_df.to_excel('Forestmodel Result.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy import stats\n",
    "\n",
    "# Kendall rank correlation coefficient\n",
    "def Kendall_rank(model_function, Trainingdataset, Labels):\n",
    "    x1 = model_function.predict(Trainingdataset).ravel()\n",
    "    x2 = Labels.ravel()\n",
    "    tau, p_value = stats.kendalltau(x1, x2)\n",
    "    return tau, p_value\n",
    "\n",
    "def display_scores(model_function):\n",
    "    scores = cross_val_score(model_function, Train, Train_labels,\n",
    "    scoring=\"neg_mean_squared_error\", cv=10)\n",
    "    Kendall_tau, Kendall_p_value = Kendall_rank(model_function, Train, Train_labels)\n",
    "    Rmse_scores = np.sqrt(-scores)\n",
    "    print(f\"----------{model_function}----------\")\n",
    "    print(\"Scores:\", Rmse_scores)\n",
    "    print(\"Mean:\", Rmse_scores.mean())\n",
    "    print(\"Standard deviation:\", Rmse_scores.std())\n",
    "    print(\"Kendall tau: \", Kendall_tau)\n",
    "    print(\"Kendall p value: \", Kendall_p_value)\n",
    "\n",
    "display_scores(tree_reg)\n",
    "display_scores(lin_reg)\n",
    "display_scores(forest_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Saving the model parameters\n",
    "def save_model(model_function):\n",
    "    model_path = r\"Model backup\"\n",
    "    path = os.path.join(model_path, f\"{model_function}.pkl\")\n",
    "    joblib.dump(model_function, path)\n",
    "    \n",
    "def load_model(model_function):\n",
    "    model_path = r\"Model backup\"\n",
    "    path = os.path.join(model_path, f\"{model_function}.pkl\")\n",
    "    my_model_loaded = joblib.load(path)\n",
    "    return my_model_loaded\n",
    "\n",
    "save_model(tree_reg)\n",
    "save_model(lin_reg)\n",
    "save_model(forest_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trial run the Logistic Regression model, should give you a failed message\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "\n",
    "lab_enc = preprocessing.LabelEncoder()\n",
    "# encoded = lab_enc.fit_transform(trainingScores)\n",
    "\n",
    "Binning = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\n",
    "log_Train = Binning.fit_transform(Train)\n",
    "print(utils.multiclass.type_of_target(log_Train))\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_Train_labels = Trial_train[\"Default\"].astype(int).copy().to_numpy()\n",
    "print(utils.multiclass.type_of_target(log_Train_labels))\n",
    "\n",
    "log_reg.fit(log_Train.astype('int'), log_Train_labels.astype('int'))\n",
    "\n",
    "def display_scores(model_function):\n",
    "    scores = cross_val_score(model_function, log_Train, Train_labels,\n",
    "    scoring=\"neg_mean_squared_error\", cv=10)\n",
    "    Rmse_scores = np.sqrt(-scores)\n",
    "    print(f\"----------{model_function}----------\")\n",
    "    print(\"Scores:\", Rmse_scores)\n",
    "    print(\"Mean:\", Rmse_scores.mean())\n",
    "    print(\"Standard deviation:\", Rmse_scores.std())\n",
    "    \n",
    "display_scores(log_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the step by step implementation of a logistic regression with gradient boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A full implementation of Logisticregression with class\n",
    "class Logisticregression():\n",
    "    def __init__(self):\n",
    "        self.weight = []\n",
    "        self.lr = 0.02\n",
    "        self.iters = 10000\n",
    "    # Cost function of the algorithm using the absolute log distance of the predictions/output from the expected output\n",
    "    def cost_function(self, features, labels, weights):\n",
    "\n",
    "    #     Using Mean Absolute Error\n",
    "\n",
    "    #     Features:(100,3)\n",
    "    #     Labels: (100,1)\n",
    "    #     Weights:(3,1)\n",
    "    #     Returns 1D matrix of predictions\n",
    "    #     Cost = (labels*log(predictions) + (1-labels)*log(1-predictions) ) / len(labels)\n",
    "\n",
    "        observations = len(labels)\n",
    "        predictions = self.prediction(features, weights)\n",
    "        #Take the error when label=1\n",
    "        class1_cost = -labels*np.log(predictions)\n",
    "        #Take the error when label=0\n",
    "        class2_cost = (1-labels)*np.log(1-predictions)\n",
    "        #Take the sum of both costs\n",
    "        cost = class1_cost - class2_cost\n",
    "        #Take the average cost\n",
    "        cost = cost.sum() / observations\n",
    "        return cost\n",
    "\n",
    "    def update_weights(self, features, labels, weights, lr):\n",
    "    #     Vectorized Gradient Descent\n",
    "    #     Features:(200, 3)\n",
    "    #     Labels: (200, 1)\n",
    "    #     Weights:(3, 1)\n",
    "        N = len(features)\n",
    "        #1 - Get Predictions\n",
    "        predictions = self.prediction(features, weights)\n",
    "        #2 Transpose features from (200, 3) to (3, 200)\n",
    "        # So we can multiply w the (200,1)  cost matrix.\n",
    "        # Returns a (3,1) matrix holding 3 partial derivatives --\n",
    "        # one for each feature -- representing the aggregate\n",
    "        # slope of the cost function across all observations\n",
    "        gradient = np.dot(features.T,  predictions - labels)\n",
    "        #3 Take the average cost derivative for each feature\n",
    "        gradient /= N\n",
    "        #4 - Multiply the gradient by our learning rate\n",
    "        gradient *= lr\n",
    "        #5 - Subtract from our weights to minimize cost\n",
    "        weights -= gradient\n",
    "        return weights\n",
    "\n",
    "\n",
    "    def decision_boundary(self, prob):\n",
    "        return 1 if prob >= .5 else 0\n",
    "    \n",
    "    def classify(self, predictions):\n",
    "    #   input  - N element array of predictions between 0 and 1\n",
    "    #   output - N element array of 0s (False) and 1s (True)\n",
    "        for i in range(0,len(predictions)):\n",
    "            predictions[i] = self.decision_boundary(predictions[i])\n",
    "        return predictions.astype('int')\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1.0 / (1 + np.exp(-z))\n",
    "\n",
    "    def prediction(self, features, weights):\n",
    "    #   Returns 1D array of probabilities\n",
    "    #   that the class label == 1\n",
    "        z = np.dot(features, weights)\n",
    "        return self.sigmoid(z)\n",
    "\n",
    "    def train(self, features, labels, weights, lr, iters):\n",
    "        cost_history = []\n",
    "        for i in range(iters):\n",
    "            weights = self.update_weights(features, labels, weights, lr)\n",
    "\n",
    "            #Calculate error for auditing purposes\n",
    "            cost = self.cost_function(features, labels, weights)\n",
    "            cost_history.append(cost)\n",
    "\n",
    "            # Log Progress\n",
    "            if i % 1000 == 0:\n",
    "                print(\"iter: \"+str(i) + \" cost: \"+str(cost))\n",
    "\n",
    "        return weights, cost_history\n",
    "    # function to fit features and get the model trained\n",
    "    def fit(self, features, labels):\n",
    "        weight = self.weight\n",
    "        iters = self.iters\n",
    "        lr = 0.05\n",
    "        if len(weight) == 0:\n",
    "            self.weight = np.zeros(16).astype('float')\n",
    "        else:\n",
    "            self.weight = weight\n",
    "        \n",
    "        cost = 1\n",
    "        for i in range (0,10):\n",
    "            weight_trial = np.random.uniform(low=-5, high=5, size=(16,)) #to be supplemented\n",
    "            weight_trial, cost_history = self.train(features, labels, weight_trial, 5, iters)\n",
    "            cost_trial = self.cost_function(features, labels, weight_trial)\n",
    "            if cost > cost_trial:\n",
    "                cost = cost_trial\n",
    "                self.weight = weight_trial\n",
    "        \n",
    "        self.weight, cost_history = self.train(features, labels, self.weight, 5, 100000)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, features):\n",
    "        weight = self.weight\n",
    "        predictions = self.prediction(features, weight)\n",
    "        return self.classify(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call from class Logisticregression to perform model training\n",
    "log_reg_1 = Logisticregression()\n",
    "log_Train_labels = Trial_train[\"Default\"].astype(int).copy().to_numpy()\n",
    "log_reg_1.fit(Train, log_Train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test = Trial_test.drop([\"Default\",\"PD\",\"Ticker\"],axis = 1).to_numpy()\n",
    "log_reg_1.predict(Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test = Trial_test.drop([\"Default\",\"PD\",\"Ticker\"],axis = 1).to_numpy()\n",
    "Test_labels = Trial_test[\"Default\"].astype(int).copy().to_numpy()\n",
    "PD_logprediction = log_reg_1.predict(Test)\n",
    "log_result_df = pd.concat([Trial_test.drop([\"PD\"],axis = 1).reset_index(drop=True), pd.DataFrame(PD_logprediction, columns=[\"PD_logprediction\"]).astype(bool).reset_index(drop=True)], axis = 1)\n",
    "log_result_df.to_excel('Logregmodel Result.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy import stats\n",
    "\n",
    "# Kendall rank correlation coefficient\n",
    "def Kendall_rank(model_function, Trainingdataset, Labels):\n",
    "    x1 = model_function.predict(Trainingdataset).ravel()\n",
    "    x2 = Labels.ravel()\n",
    "    tau, p_value = stats.kendalltau(x1, x2)\n",
    "    return tau, p_value\n",
    "\n",
    "# display and print the test scores of all models\n",
    "def display_test_scores(model_function):\n",
    "    scores = cross_val_score(model_function, Test, Test_labels,\n",
    "    scoring=\"neg_mean_squared_error\", cv=10)\n",
    "    Kendall_tau, Kendall_p_value = Kendall_rank(model_function, Test, Test_labels)\n",
    "    Rmse_scores = np.sqrt(-scores)\n",
    "    print(f\"----------{model_function}----------\")\n",
    "    print(\"Scores:\", Rmse_scores)\n",
    "    print(\"Mean:\", Rmse_scores.mean())\n",
    "    print(\"Standard deviation:\", Rmse_scores.std())\n",
    "    print(\"Kendall tau: \", Kendall_tau)\n",
    "    print(\"Kendall p value: \", Kendall_p_value)\n",
    "\n",
    "display_test_scores(tree_reg)\n",
    "display_test_scores(lin_reg)\n",
    "display_test_scores(forest_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not yet finished\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the results of each elements for linear regression\n",
    "\n",
    "# Indexing the list to get index for columns\n",
    "List = [[i for i in range(0,len(Test))],[15 for i in range(0,len(Test))]]\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.scatter(Test[List], Test_labels, s=20, edgecolor=\"black\",\n",
    "            c=\"darkorange\", label=\"data\")\n",
    "# change the PD_prediction by adding tree, log, e.g. PD_treeprediction to get other plots\n",
    "plt.plot(Test[List], PD_prediction, color=\"cornflowerblue\",\n",
    "         label=\"max_depth=2\", linewidth=0.1)\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"PD\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:41247 Train_1 size:10207\n",
      "[0]\ttrain-map:0.00960\teval-map:0.00293\n",
      "[1]\ttrain-map:0.01149\teval-map:0.00255\n",
      "[2]\ttrain-map:0.01224\teval-map:0.00262\n",
      "[3]\ttrain-map:0.01247\teval-map:0.00262\n",
      "[4]\ttrain-map:0.01370\teval-map:0.00255\n",
      "[5]\ttrain-map:0.01434\teval-map:0.00266\n",
      "[6]\ttrain-map:0.01424\teval-map:0.00272\n",
      "[7]\ttrain-map:0.01512\teval-map:0.00286\n",
      "[8]\ttrain-map:0.01562\teval-map:0.00298\n",
      "[9]\ttrain-map:0.01580\teval-map:0.00301\n",
      "[10]\ttrain-map:0.01589\teval-map:0.00295\n",
      "[11]\ttrain-map:0.01625\teval-map:0.00299\n",
      "[12]\ttrain-map:0.01635\teval-map:0.00299\n",
      "[13]\ttrain-map:0.01664\teval-map:0.00309\n",
      "[14]\ttrain-map:0.01689\teval-map:0.00322\n",
      "[15]\ttrain-map:0.01692\teval-map:0.00317\n",
      "[16]\ttrain-map:0.01727\teval-map:0.00335\n",
      "[17]\ttrain-map:0.01749\teval-map:0.00345\n",
      "[18]\ttrain-map:0.01779\teval-map:0.00340\n",
      "[19]\ttrain-map:0.01789\teval-map:0.00353\n",
      "[20]\ttrain-map:0.01817\teval-map:0.00299\n",
      "[21]\ttrain-map:0.01846\teval-map:0.00302\n",
      "[22]\ttrain-map:0.01843\teval-map:0.00299\n",
      "[23]\ttrain-map:0.01856\teval-map:0.00302\n",
      "[24]\ttrain-map:0.01873\teval-map:0.00309\n",
      "[25]\ttrain-map:0.01876\teval-map:0.00308\n",
      "[26]\ttrain-map:0.01877\teval-map:0.00312\n",
      "[27]\ttrain-map:0.01895\teval-map:0.00315\n",
      "[28]\ttrain-map:0.01908\teval-map:0.00319\n",
      "[29]\ttrain-map:0.01930\teval-map:0.00316\n",
      "[30]\ttrain-map:0.01946\teval-map:0.00320\n",
      "[31]\ttrain-map:0.01947\teval-map:0.00323\n",
      "[32]\ttrain-map:0.01980\teval-map:0.00323\n",
      "[33]\ttrain-map:0.01983\teval-map:0.00325\n",
      "[34]\ttrain-map:0.01984\teval-map:0.00328\n",
      "[35]\ttrain-map:0.01984\teval-map:0.00327\n",
      "[36]\ttrain-map:0.02026\teval-map:0.00329\n",
      "[37]\ttrain-map:0.02035\teval-map:0.00329\n",
      "[38]\ttrain-map:0.02044\teval-map:0.00316\n",
      "[39]\ttrain-map:0.02062\teval-map:0.00321\n",
      "[40]\ttrain-map:0.02059\teval-map:0.00324\n",
      "[41]\ttrain-map:0.02080\teval-map:0.00334\n",
      "[42]\ttrain-map:0.02115\teval-map:0.00332\n",
      "[43]\ttrain-map:0.02135\teval-map:0.00333\n",
      "[44]\ttrain-map:0.02139\teval-map:0.00316\n",
      "[45]\ttrain-map:0.02131\teval-map:0.00324\n",
      "[46]\ttrain-map:0.02139\teval-map:0.00326\n",
      "[47]\ttrain-map:0.02137\teval-map:0.00317\n",
      "[48]\ttrain-map:0.02157\teval-map:0.00320\n",
      "[49]\ttrain-map:0.02171\teval-map:0.00324\n",
      "[50]\ttrain-map:0.02169\teval-map:0.00326\n",
      "[51]\ttrain-map:0.02164\teval-map:0.00324\n",
      "[52]\ttrain-map:0.02187\teval-map:0.00323\n",
      "[53]\ttrain-map:0.02175\teval-map:0.00325\n",
      "[54]\ttrain-map:0.02185\teval-map:0.00332\n",
      "[55]\ttrain-map:0.02202\teval-map:0.00335\n",
      "[56]\ttrain-map:0.02212\teval-map:0.00337\n",
      "[57]\ttrain-map:0.02224\teval-map:0.00343\n",
      "[58]\ttrain-map:0.02253\teval-map:0.00337\n",
      "[59]\ttrain-map:0.02263\teval-map:0.00341\n",
      "[60]\ttrain-map:0.02291\teval-map:0.00338\n",
      "[61]\ttrain-map:0.02315\teval-map:0.00331\n",
      "[62]\ttrain-map:0.02323\teval-map:0.00335\n",
      "[63]\ttrain-map:0.02336\teval-map:0.00339\n",
      "[64]\ttrain-map:0.02334\teval-map:0.00336\n",
      "[65]\ttrain-map:0.02356\teval-map:0.00338\n",
      "[66]\ttrain-map:0.02384\teval-map:0.00340\n",
      "[67]\ttrain-map:0.02378\teval-map:0.00339\n",
      "[68]\ttrain-map:0.02385\teval-map:0.00335\n",
      "[69]\ttrain-map:0.02410\teval-map:0.00333\n",
      "[70]\ttrain-map:0.02408\teval-map:0.00335\n",
      "[71]\ttrain-map:0.02395\teval-map:0.00338\n",
      "[72]\ttrain-map:0.02405\teval-map:0.00338\n",
      "[73]\ttrain-map:0.02411\teval-map:0.00339\n",
      "[74]\ttrain-map:0.02391\teval-map:0.00336\n",
      "[75]\ttrain-map:0.02400\teval-map:0.00338\n",
      "[76]\ttrain-map:0.02423\teval-map:0.00339\n",
      "[77]\ttrain-map:0.02453\teval-map:0.00332\n",
      "[78]\ttrain-map:0.02477\teval-map:0.00319\n",
      "[79]\ttrain-map:0.02439\teval-map:0.00323\n",
      "[80]\ttrain-map:0.02434\teval-map:0.00326\n",
      "[81]\ttrain-map:0.02452\teval-map:0.00328\n",
      "[82]\ttrain-map:0.02447\teval-map:0.00330\n",
      "[83]\ttrain-map:0.02456\teval-map:0.00335\n",
      "[84]\ttrain-map:0.02482\teval-map:0.00336\n",
      "[85]\ttrain-map:0.02484\teval-map:0.00338\n",
      "[86]\ttrain-map:0.02518\teval-map:0.00335\n",
      "[87]\ttrain-map:0.02550\teval-map:0.00337\n",
      "[88]\ttrain-map:0.02562\teval-map:0.00339\n",
      "[89]\ttrain-map:0.02567\teval-map:0.00342\n",
      "[90]\ttrain-map:0.02601\teval-map:0.00343\n",
      "[91]\ttrain-map:0.02633\teval-map:0.00348\n",
      "[92]\ttrain-map:0.02657\teval-map:0.00352\n",
      "[93]\ttrain-map:0.02692\teval-map:0.00357\n",
      "[94]\ttrain-map:0.02699\teval-map:0.00357\n",
      "[95]\ttrain-map:0.02684\teval-map:0.00360\n",
      "[96]\ttrain-map:0.02672\teval-map:0.00362\n",
      "[97]\ttrain-map:0.02666\teval-map:0.00362\n",
      "[98]\ttrain-map:0.02665\teval-map:0.00365\n",
      "[99]\ttrain-map:0.02668\teval-map:0.00358\n",
      "[100]\ttrain-map:0.02698\teval-map:0.00364\n",
      "[101]\ttrain-map:0.02720\teval-map:0.00358\n",
      "[102]\ttrain-map:0.02722\teval-map:0.00363\n",
      "[103]\ttrain-map:0.02732\teval-map:0.00359\n",
      "[104]\ttrain-map:0.02728\teval-map:0.00366\n",
      "[105]\ttrain-map:0.02748\teval-map:0.00367\n",
      "[106]\ttrain-map:0.02761\teval-map:0.00370\n",
      "[107]\ttrain-map:0.02799\teval-map:0.00364\n",
      "[108]\ttrain-map:0.02800\teval-map:0.00364\n",
      "[109]\ttrain-map:0.02809\teval-map:0.00366\n",
      "[110]\ttrain-map:0.02819\teval-map:0.00376\n",
      "[111]\ttrain-map:0.02804\teval-map:0.00361\n",
      "[112]\ttrain-map:0.02830\teval-map:0.00375\n",
      "[113]\ttrain-map:0.02833\teval-map:0.00368\n",
      "[114]\ttrain-map:0.02837\teval-map:0.00361\n",
      "[115]\ttrain-map:0.02830\teval-map:0.00361\n",
      "[116]\ttrain-map:0.02813\teval-map:0.00355\n",
      "[117]\ttrain-map:0.02817\teval-map:0.00360\n",
      "[118]\ttrain-map:0.02858\teval-map:0.00366\n",
      "[119]\ttrain-map:0.02836\teval-map:0.00375\n",
      "[120]\ttrain-map:0.02854\teval-map:0.00393\n",
      "[121]\ttrain-map:0.02854\teval-map:0.00380\n",
      "[122]\ttrain-map:0.02849\teval-map:0.00393\n",
      "[123]\ttrain-map:0.02850\teval-map:0.00398\n",
      "[124]\ttrain-map:0.02822\teval-map:0.00404\n",
      "[125]\ttrain-map:0.02813\teval-map:0.00401\n",
      "[126]\ttrain-map:0.02834\teval-map:0.00400\n",
      "[127]\ttrain-map:0.02847\teval-map:0.00395\n",
      "[128]\ttrain-map:0.02871\teval-map:0.00401\n",
      "[129]\ttrain-map:0.02895\teval-map:0.00409\n",
      "[130]\ttrain-map:0.02909\teval-map:0.00407\n",
      "[131]\ttrain-map:0.02939\teval-map:0.00416\n",
      "[132]\ttrain-map:0.02945\teval-map:0.00410\n",
      "[133]\ttrain-map:0.02964\teval-map:0.00416\n",
      "[134]\ttrain-map:0.02952\teval-map:0.00418\n",
      "[135]\ttrain-map:0.02953\teval-map:0.00420\n",
      "[136]\ttrain-map:0.02955\teval-map:0.00418\n",
      "[137]\ttrain-map:0.02936\teval-map:0.00412\n",
      "[138]\ttrain-map:0.02953\teval-map:0.00416\n",
      "[139]\ttrain-map:0.02978\teval-map:0.00405\n",
      "[140]\ttrain-map:0.02994\teval-map:0.00408\n",
      "[141]\ttrain-map:0.03000\teval-map:0.00417\n",
      "[142]\ttrain-map:0.03002\teval-map:0.00428\n",
      "[143]\ttrain-map:0.03005\teval-map:0.00419\n",
      "[144]\ttrain-map:0.03004\teval-map:0.00417\n",
      "[145]\ttrain-map:0.03055\teval-map:0.00401\n",
      "[146]\ttrain-map:0.03065\teval-map:0.00399\n",
      "[147]\ttrain-map:0.03097\teval-map:0.00397\n",
      "[148]\ttrain-map:0.03060\teval-map:0.00395\n",
      "[149]\ttrain-map:0.03079\teval-map:0.00401\n",
      "[150]\ttrain-map:0.03098\teval-map:0.00403\n",
      "[151]\ttrain-map:0.03109\teval-map:0.00402\n",
      "[152]\ttrain-map:0.03113\teval-map:0.00406\n",
      "[153]\ttrain-map:0.03140\teval-map:0.00404\n",
      "[154]\ttrain-map:0.03141\teval-map:0.00412\n",
      "[155]\ttrain-map:0.03167\teval-map:0.00407\n",
      "[156]\ttrain-map:0.03183\teval-map:0.00410\n",
      "[157]\ttrain-map:0.03206\teval-map:0.00397\n",
      "[158]\ttrain-map:0.03181\teval-map:0.00392\n",
      "[159]\ttrain-map:0.03190\teval-map:0.00404\n",
      "[160]\ttrain-map:0.03176\teval-map:0.00407\n",
      "[161]\ttrain-map:0.03211\teval-map:0.00415\n",
      "[162]\ttrain-map:0.03213\teval-map:0.00418\n",
      "[163]\ttrain-map:0.03234\teval-map:0.00417\n",
      "[164]\ttrain-map:0.03269\teval-map:0.00408\n",
      "[165]\ttrain-map:0.03301\teval-map:0.00400\n",
      "[166]\ttrain-map:0.03293\teval-map:0.00399\n",
      "[167]\ttrain-map:0.03294\teval-map:0.00403\n",
      "[168]\ttrain-map:0.03302\teval-map:0.00402\n",
      "[169]\ttrain-map:0.03342\teval-map:0.00409\n",
      "[170]\ttrain-map:0.03347\teval-map:0.00408\n",
      "[171]\ttrain-map:0.03344\teval-map:0.00417\n",
      "[172]\ttrain-map:0.03368\teval-map:0.00419\n",
      "[173]\ttrain-map:0.03388\teval-map:0.00410\n",
      "[174]\ttrain-map:0.03365\teval-map:0.00408\n",
      "[175]\ttrain-map:0.03361\teval-map:0.00409\n",
      "[176]\ttrain-map:0.03383\teval-map:0.00416\n",
      "[177]\ttrain-map:0.03368\teval-map:0.00411\n",
      "[178]\ttrain-map:0.03392\teval-map:0.00411\n",
      "[179]\ttrain-map:0.03399\teval-map:0.00413\n",
      "[180]\ttrain-map:0.03401\teval-map:0.00416\n",
      "[181]\ttrain-map:0.03410\teval-map:0.00414\n",
      "[182]\ttrain-map:0.03418\teval-map:0.00413\n",
      "[183]\ttrain-map:0.03435\teval-map:0.00414\n",
      "[184]\ttrain-map:0.03483\teval-map:0.00416\n",
      "[185]\ttrain-map:0.03507\teval-map:0.00391\n",
      "[186]\ttrain-map:0.03545\teval-map:0.00392\n",
      "[187]\ttrain-map:0.03582\teval-map:0.00382\n",
      "[188]\ttrain-map:0.03583\teval-map:0.00387\n",
      "[189]\ttrain-map:0.03605\teval-map:0.00387\n",
      "[190]\ttrain-map:0.03633\teval-map:0.00381\n",
      "[191]\ttrain-map:0.03662\teval-map:0.00385\n",
      "[192]\ttrain-map:0.03671\teval-map:0.00375\n",
      "[193]\ttrain-map:0.03680\teval-map:0.00372\n",
      "[194]\ttrain-map:0.03677\teval-map:0.00372\n",
      "[195]\ttrain-map:0.03708\teval-map:0.00360\n",
      "[196]\ttrain-map:0.03711\teval-map:0.00364\n",
      "[197]\ttrain-map:0.03759\teval-map:0.00375\n",
      "[198]\ttrain-map:0.03765\teval-map:0.00376\n",
      "[199]\ttrain-map:0.03744\teval-map:0.00376\n",
      "[200]\ttrain-map:0.03740\teval-map:0.00372\n",
      "[201]\ttrain-map:0.03710\teval-map:0.00371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[202]\ttrain-map:0.03726\teval-map:0.00365\n",
      "[203]\ttrain-map:0.03738\teval-map:0.00366\n",
      "[204]\ttrain-map:0.03744\teval-map:0.00367\n",
      "[205]\ttrain-map:0.03780\teval-map:0.00368\n",
      "[206]\ttrain-map:0.03815\teval-map:0.00369\n",
      "[207]\ttrain-map:0.03817\teval-map:0.00369\n",
      "[208]\ttrain-map:0.03845\teval-map:0.00371\n",
      "[209]\ttrain-map:0.03853\teval-map:0.00371\n",
      "[210]\ttrain-map:0.03888\teval-map:0.00368\n",
      "[211]\ttrain-map:0.03923\teval-map:0.00369\n",
      "[212]\ttrain-map:0.03948\teval-map:0.00370\n",
      "[213]\ttrain-map:0.03966\teval-map:0.00370\n",
      "[214]\ttrain-map:0.03983\teval-map:0.00372\n",
      "[215]\ttrain-map:0.04055\teval-map:0.00371\n",
      "[216]\ttrain-map:0.04058\teval-map:0.00378\n",
      "[217]\ttrain-map:0.04056\teval-map:0.00378\n",
      "[218]\ttrain-map:0.04059\teval-map:0.00378\n",
      "[219]\ttrain-map:0.04070\teval-map:0.00377\n",
      "[220]\ttrain-map:0.04123\teval-map:0.00375\n",
      "[221]\ttrain-map:0.04097\teval-map:0.00375\n",
      "[222]\ttrain-map:0.04114\teval-map:0.00383\n",
      "[223]\ttrain-map:0.04095\teval-map:0.00388\n",
      "[224]\ttrain-map:0.04148\teval-map:0.00385\n",
      "[225]\ttrain-map:0.04216\teval-map:0.00387\n",
      "[226]\ttrain-map:0.04233\teval-map:0.00386\n",
      "[227]\ttrain-map:0.04252\teval-map:0.00392\n",
      "[228]\ttrain-map:0.04267\teval-map:0.00386\n",
      "[229]\ttrain-map:0.04302\teval-map:0.00390\n",
      "[230]\ttrain-map:0.04300\teval-map:0.00396\n",
      "[231]\ttrain-map:0.04279\teval-map:0.00399\n",
      "[232]\ttrain-map:0.04290\teval-map:0.00397\n",
      "[233]\ttrain-map:0.04340\teval-map:0.00394\n",
      "[234]\ttrain-map:0.04383\teval-map:0.00396\n",
      "[235]\ttrain-map:0.04408\teval-map:0.00387\n",
      "[236]\ttrain-map:0.04455\teval-map:0.00386\n",
      "[237]\ttrain-map:0.04463\teval-map:0.00385\n",
      "[238]\ttrain-map:0.04472\teval-map:0.00390\n",
      "[239]\ttrain-map:0.04491\teval-map:0.00388\n",
      "[240]\ttrain-map:0.04461\teval-map:0.00386\n",
      "[241]\ttrain-map:0.04474\teval-map:0.00385\n",
      "[242]\ttrain-map:0.04480\teval-map:0.00384\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "train_split = np.random.rand(len(Trial_train)) < 0.8\n",
    "Trial_train_1 = Trial_train[train_split]\n",
    "Trial_train_2 = Trial_train[~train_split]\n",
    "print(f'Train size:{len(Trial_train_1)} Train_1 size:{len(Trial_train_2)}')\n",
    "\n",
    "xgTrain_1 = Trial_train_1.drop([\"Default\",\"PD\",\"Ticker\"],axis = 1).to_numpy()\n",
    "xgTrain_labels_1 = Trial_train_1[\"PD\"].copy().to_numpy()\n",
    "xgTrain_2 = Trial_train_2.drop([\"Default\",\"PD\",\"Ticker\"],axis = 1).to_numpy()\n",
    "xgTrain_labels_2 = Trial_train_2[\"PD\"].copy().to_numpy()\n",
    "#xgTrain_labels = np.where(Trial_train[\"PD\"].copy().to_numpy()>0.1, 1, 0)\n",
    "xgTest = Trial_test.drop([\"Default\",\"PD\",\"Ticker\"],axis = 1).to_numpy()\n",
    "xgTest_labels = Trial_test[\"PD\"].copy().to_numpy()\n",
    "#xgTest_labels = np.where(Trial_test[\"PD\"].copy().to_numpy()>0.1, 1, 0)\n",
    "dtrain_1 = xgb.DMatrix(xgTrain_1, label=xgTrain_labels_1)\n",
    "dtrain_2 = xgb.DMatrix(xgTrain_2, label=xgTrain_labels_2)\n",
    "\n",
    "param = {'max_depth': 50, 'eta': 0.1, 'objective': 'rank:pairwise'}\n",
    "\n",
    "# specify validations set to watch performance\n",
    "watchlist = [  (dtrain_1, 'train'), (dtrain_2, 'eval')]\n",
    "num_round = 500\n",
    "bst_2 = xgb.train(param, dtrain_1, num_round, watchlist, early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Training Sets------\n",
      "Kendall tau:  0.8592945918471063\n",
      "Kendall p value:  0.0\n",
      "-----Training Sets_1------\n",
      "Kendall tau:  0.6200395110964915\n",
      "Kendall p value:  0.0\n",
      "-----Testing Sets------\n",
      "Kendall tau:  0.6163882832021066\n",
      "Kendall p value:  0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy import stats\n",
    "# this is prediction\n",
    "def Kendall_rank_Tree(Trainingdataset, Labels):\n",
    "    x1 = bst_2.predict(xgb.DMatrix(Trainingdataset, label=Labels), ntree_limit=bst_2.best_ntree_limit).ravel()\n",
    "    x2 = Labels.ravel()\n",
    "    tau, p_value = stats.kendalltau(x1, x2)\n",
    "    return tau, p_value\n",
    "\n",
    "Kendall_tau, Kendall_p_value = Kendall_rank_Tree(xgTrain_1, xgTrain_labels_1)\n",
    "print(\"-----Training Sets------\")\n",
    "print(\"Kendall tau: \", Kendall_tau)\n",
    "print(\"Kendall p value: \", Kendall_p_value)\n",
    "Kendall_tau, Kendall_p_value = Kendall_rank_Tree(xgTrain_2, xgTrain_labels_2)\n",
    "print(\"-----Training Sets_1------\")\n",
    "print(\"Kendall tau: \", Kendall_tau)\n",
    "print(\"Kendall p value: \", Kendall_p_value)\n",
    "Kendall_test_tau, Kendall_test_p_value = Kendall_rank_Tree(xgTest, xgTest_labels)\n",
    "print(\"-----Testing Sets------\")\n",
    "print(\"Kendall tau: \", Kendall_test_tau)\n",
    "print(\"Kendall p value: \", Kendall_test_p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(bst_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xgboost model result and export to excel for discussion\n",
    "PD_xgprediction = bst_2.predict(xgb.DMatrix(xgTrain_1, label = xgTrain_labels_1), ntree_limit=bst_2.best_ntree_limit)\n",
    "\n",
    "Xg_result_df = pd.concat([Trial_train_1.drop([\"Default\"],axis = 1).reset_index(drop=True), pd.DataFrame(PD_xgprediction, columns=[\"PD_XGprediction\"]).reset_index(drop=True)], axis = 1)\n",
    "Xg_result_df.to_excel('Xgboostmodel Result.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
